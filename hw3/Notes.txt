PAGEID
pId1 = PageId(FileId(5), 100)
pId2 = PageId.unpack(pId1.pack())

SCEMA
DBSchema('employeeId', [('id', 'int')])
schema.match(DBSchema('employee2', [('id', 'int'), ('dob', 'char(10)'), ('salary', 'int')]))
schema.default()
projectBinary(self, binaryInstance, schema):
project(self, instance, schema):
instantiate(self, *args):

# Returns a new schema with renamed attributes.
  # The arguments are a new schema name and a renaming dictionary
  # from current name to new name, e.g. for a schema with fields['a', 'b']:
  # attrNameMap = {'a': 'a2', 'b': 'b2'}
  def rename(self, schemaName, attrNameMap):
    newFields = [attrNameMap[x] for x in self.fields]
    return DBSchema(schemaName, list(zip(newFields, self.types)))


OPERATOR
# Returns the number of tuples this operator produces, either
  # as an estimate or a profiled actual cardinality.
  def cardinality(self, estimated):
    if estimated:
      return self.estimatedCardinality * self.sampleFactor
    else:
      return self.actualCardinality

# Returns this operator's selectivity, either as an estimate or
  # a profiled actual selectivity.
  def selectivity(self, estimated):
    numInputs = sum(map(lambda x: x.cardinality(estimated), self.inputs()))
    numOutputs = self.cardinality(estimated)
    return numOutputs / numInputs

# Returns the cost of this operator in terms of a dimensionless
  # metric (e.g., number of I/Os, CPU cycles, etc). This is also either
  # as an estimate or a profiled actual cost.
  def cost(self, estimated):
    subPlanCost = sum(map(lambda x: x.cost(estimated), self.inputs()))
    return self.localCost(estimated) + subPlanCost

  def localCost(self, estimated):
    numInputs = sum(map(lambda x: x.cardinality(estimated), self.inputs()))
    return numInputs * self.tupleCost

  def loadSchema(self, schema, tupleData):
    schemaLocals = {}
    for ((k,t),v) in zip(schema.schema(), schema.unpack(tupleData)):
      schemaLocals[k] = v
    return schemaLocals

  # Instructs this operator to perform sampling during execution.
  # This propagates the sampling rate over all of our children.
  def useSampling(self, sampled, sampleFactor):
    self.sampled = sampled
    self.sampleFactor = sampleFactor
    for childOp in self.inputs():
      childOp.useSampling(sampled, sampleFactor)

PLAN
  # Returns the query result schema.
  def schema(self):
    return self.root.schema()

  # Returns the cost of the plan, either as an estimate or as an actual cost
  # based on the boolean 'estimated' parameter.
  #
  # For the actual cost, each operator should determine its own local cost added to the
  # cost of its children.
  def cost(self, estimated):
    return self.root.cost(estimated)

  # Sample-based statistics estimation, taking the desired sampling ratio as an argument.
  # This configures all operators in the plan to use sampling, and then runs the query plan.
  # Each operator tracks its estimated statistics during execution while in sampling mode.
  # We iterate over all tuples produced as the sampled query result, counting the result
  # cardinality. This cardinality is scaled up by the given factor to match the
  # original dataset from which the sample was taken, that is:
  #
  #     scaleFactor = actual dataset size / desired sample dataset size
  #
  def sample(self, scaleFactor):
    self.root.useSampling(True, scaleFactor)
    # Process query, update each operator's cost, cardinality, and selectivity estimates.
    for page in self:
      for tup in page[1]:
        self.sampleCardinality += 1

    # Leave the scale factor unchanged, so that we can correctly use estimated statistics after sampling.
    self.root.useSampling(False, scaleFactor)
    return self.sampleCardinality * scaleFactor

  def pushdownOperators(self):
    self.root = self.root.pushdownOperators()
    return self

  """
  A query plan builder class that can be used for LINQ-like construction of queries.

  A plan builder consists of an operator field, as the running root of the query tree.
  Each method returns a plan builder instance, that can be used to further
  operators compose with additional builder methods.

  A plan builder yields a Query.Plan instance through its finalize() method.

  >>> import Database
  >>> db = Database.Database()
  >>> db.createRelation('employee', [('id', 'int'), ('age', 'int')])
  >>> schema = db.relationSchema('employee')

  # Populate relation
  >>> for tup in [schema.pack(schema.instantiate(i, 2*i+20)) for i in range(20)]:
  ...    _ = db.insertTuple(schema.name, tup)
  ...

  ### SELECT * FROM Employee WHERE age < 30
  >>> query1 = db.query().fromTable('employee').where("age < 30").finalize()

  >>> query1.relations()
  ['employee']

  >>> print(query1.explain()) # doctest: +ELLIPSIS
  Select[...,cost=...](predicate='age < 30')
    TableScan[...,cost=...](employee)

  >>> [schema.unpack(tup).age for page in db.processQuery(query1) for tup in page[1]]
  [20, 22, 24, 26, 28]


  ### SELECT eid FROM Employee WHERE age < 30
  >>> query2 = db.query().fromTable('employee').where("age < 30").select({'id': ('id', 'int')}).finalize()

  >>> print(query2.explain()) # doctest: +ELLIPSIS
  Project[...,cost=...](projections={'id': ('id', 'int')})
    Select[...,cost=...](predicate='age < 30')
      TableScan[...,cost=...](employee)

  >>> [query2.schema().unpack(tup).id for page in db.processQuery(query2) for tup in page[1]]
  [0, 1, 2, 3, 4]


  ### SELECT * FROM Employee UNION ALL Employee
  >>> query3 = db.query().fromTable('employee').union(db.query().fromTable('employee')).finalize()

  >>> print(query3.explain()) # doctest: +ELLIPSIS
  UnionAll[...,cost=...]
    TableScan[...,cost=...](employee)
    TableScan[...,cost=...](employee)

  >>> [query3.schema().unpack(tup).id for page in db.processQuery(query3) for tup in page[1]] # doctest:+ELLIPSIS
  [0, 1, 2, ..., 19, 0, 1, 2, ..., 19]

  ### SELECT * FROM Employee E1 JOIN Employee E2 ON E1.id = E2.id
  >>> e2schema = schema.rename('employee2', {'id':'id2', 'age':'age2'})

  >>> query4 = db.query().fromTable('employee').join( \
        db.query().fromTable('employee'), \
        rhsSchema=e2schema, \
        method='block-nested-loops', expr='id == id2').finalize()

  >>> print(query4.explain()) # doctest: +ELLIPSIS
  BNLJoin[...,cost=...](expr='id == id2')
    TableScan[...,cost=...](employee)
    TableScan[...,cost=...](employee)

  >>> q4results = [query4.schema().unpack(tup) for page in db.processQuery(query4) for tup in page[1]]
  >>> [(tup.id, tup.id2) for tup in q4results] # doctest:+ELLIPSIS
  [(0, 0), (1, 1), (2, 2), ..., (18, 18), (19, 19)]

  ### Hash join test with the same query.
  ### SELECT * FROM Employee E1 JOIN Employee E2 ON E1.id = E2.id
  >>> e2schema   = schema.rename('employee2', {'id':'id2', 'age':'age2'})
  >>> keySchema  = DBSchema('employeeKey',  [('id', 'int')])
  >>> keySchema2 = DBSchema('employeeKey2', [('id2', 'int')])

  >>> query5 = db.query().fromTable('employee').join( \
          db.query().fromTable('employee'), \
          rhsSchema=e2schema, \
          method='hash', \
          lhsHashFn='hash(id) % 4',  lhsKeySchema=keySchema, \
          rhsHashFn='hash(id2) % 4', rhsKeySchema=keySchema2, \
        ).finalize()

  >>> print(query5.explain()) # doctest: +ELLIPSIS
  HashJoin[...,cost=...](lhsKeySchema=employeeKey[(id,int)],rhsKeySchema=employeeKey2[(id2,int)],lhsHashFn='hash(id) % 4',rhsHashFn='hash(id2) % 4')
    TableScan[...,cost=...](employee)
    TableScan[...,cost=...](employee)

  >>> q5results = [query5.schema().unpack(tup) for page in db.processQuery(query5) for tup in page[1]]
  >>> sorted([(tup.id, tup.id2) for tup in q5results]) # doctest:+ELLIPSIS
  [(0, 0), (1, 1), (2, 2), ..., (18, 18), (19, 19)]

  ### Group by aggregate query
  ### SELECT id, max(age) FROM Employee GROUP BY id
  >>> aggMinMaxSchema = DBSchema('minmax', [('minAge', 'int'), ('maxAge','int')])
  >>> query6 = db.query().fromTable('employee').groupBy( \
          groupSchema=keySchema, \
          aggSchema=aggMinMaxSchema, \
          groupExpr=(lambda e: e.id), \
          aggExprs=[(sys.maxsize, lambda acc, e: min(acc, e.age), lambda x: x), \
                    (0, lambda acc, e: max(acc, e.age), lambda x: x)], \
          groupHashFn=(lambda gbVal: hash(gbVal[0]) % 2) \
        ).finalize()

  >>> print(query6.explain()) # doctest: +ELLIPSIS
  GroupBy[...,cost=...](groupSchema=employeeKey[(id,int)], aggSchema=minmax[(minAge,int),(maxAge,int)])
    TableScan[...,cost=...](employee)

  >>> q6results = [query6.schema().unpack(tup) for page in db.processQuery(query6) for tup in page[1]]
  >>> sorted([(tup.id, tup.minAge, tup.maxAge) for tup in q6results]) # doctest:+ELLIPSIS
  [(0, 20, 20), (1, 22, 22), ..., (18, 56, 56), (19, 58, 58)]

  # Populate employees relation with another 10000 tuples
  >>> for tup in [schema.pack(schema.instantiate(i, math.ceil(random.gauss(45, 25)))) for i in range(10000)]:
  ...    _ = db.insertTuple(schema.name, tup)
  ...

  ### Sample 1/10th of: SELECT * FROM Employee WHERE age < 30
  >>> query8 = db.query().fromTable('employee').where("age < 30").finalize()
  >>> estimatedSize = query8.sample(10)
  >>> estimatedSize > 0
  True


